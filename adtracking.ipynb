{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import imblearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in train and test csvs from zipfile\n",
    "zip_file = zipfile.ZipFile('data/adtracking.zip')\n",
    "dfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()\n",
    "       if text_file.filename == 'train.csv' or text_file.filename =='test.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    184447044\n",
       "1       456846\n",
       "Name: is_attributed, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can see that our classes are severely imbalanced.  We will start by undersamping the majority class to equalize\n",
    "# May return and compare to SMOTE or other approaches depending on model performance\n",
    "dfs['train.csv'].is_attributed.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We undersample first because while there is a huge class imbalance, there's still a lot of data in the minority class.  This means that the simple approach has a good chance of being effective while reducing required compute resources.\n",
    "\n",
    "Undersampling before the split means a smaller dataset but no chance of data leakage, and since the minority class is large the drawback isn't too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0   83230    3       1  13      379  2017-11-06 14:32:21             NaN   \n",
       "1   17357    3       1  19      379  2017-11-06 14:33:34             NaN   \n",
       "2   35810    3       1  13      379  2017-11-06 14:34:12             NaN   \n",
       "3   45745   14       1  13      478  2017-11-06 14:34:52             NaN   \n",
       "4  161007    3       1  13      379  2017-11-06 14:35:08             NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['train.csv'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full X and y data from training\n",
    "X_full = dfs['train.csv'].drop(columns=['is_attributed', 'attributed_time'])\n",
    "y_full = dfs['train.csv'][['is_attributed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a random undersampler\n",
    "undersample = imblearn.under_sampling.RandomUnderSampler(sampling_strategy='majority')\n",
    "# Apply the undersampling transformation\n",
    "X_us, y_us = undersample.fit_resample(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the undersampled data into training and validation dfs, 20% of data going to validation\n",
    "test_size = 0.2\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_us, y_us, test_size=test_size, random_state=1233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll add our functions to create features.\n",
    "\n",
    "We'll add qite a few things:\n",
    "1) From click time, we want to extract the day / hour, ensuring that our times are converted to our timezone\n",
    "2) Want to find unique users by combinations of our base categories (ip, app, device, os, channel).  May want to hash these - check if feature space is large enough\n",
    "3) Click counts within next X (1? 2? both?) hour(s) by 'user' category - may want to get this for multiple 'user' categories\n",
    "4) Time to next click by 'user' category - may want to get this for multiple 'user' categories\n",
    "5) Time from last click by 'user' category - may want to get this for multiple 'user' categories\n",
    "6) Avg. attributed ratio of past clicks by 'user' category - may want to get this for multiple 'user' categories\n",
    "7) Counts of clicks by 'user' category - may want to get this for multiple 'user' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour_day_from_clicktime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds the hour and day columns as ints from the click_time column\n",
    "    Returns the input df with the hour, day columns added.  Acts on a copy of the input df.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2['hour'] = pd.to_datetime(df2['click_time']).dt.hour.astype('uint8')\n",
    "    df2['day'] = pd.to_datetime(df2['click_time']).dt.day.astype('uint8')\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to have a few combinations to try to get 'unique users' which we'll group by\n",
    "# We'll put lists of columns to group by below\n",
    "# Hard code instead of doing all combinations so we can easily comment out ones we don't want,\n",
    "# Also we don't want a lot of the combinations\n",
    "u_user_lists = [\n",
    "    # IP with every other base\n",
    "    ['ip', 'channel'],\n",
    "    ['ip', 'device'], \n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'app'],\n",
    "    # IP and time features - must be done after adding time features\n",
    "    ['ip', 'day', 'hour'],\n",
    "    # Perhaps IP isn't as important\n",
    "    ['app', 'channel'],\n",
    "    # Triplet(s)\n",
    "    ['ip', 'app', 'os'],\n",
    "    # Quartet(s)\n",
    "    ['ip', 'device', 'os', 'app']\n",
    "    # Exclude all 5 together as these will be used for grouping\n",
    "]\n",
    "\n",
    "grouping_functions = ['nunique', 'cumcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_groupby_user_features(df: pd.DataFrame, grouping_categories: List[List[str]], grouping_functions: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Takes an input dataframe, list of groupings to use, and a list of grouping functions (currently just allows for nunique and/or cumcount).\n",
    "        Adds the grouped values to a copy of the input dataframe.  \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe e.g. X_train\n",
    "        grouping_categories (List[List[str]]): List containing lists of columns to group by as strings\n",
    "        grouping_functions (List[str]): List containing strings of functions to aggregate with (must be nunique and/or cumcount at the moment)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input dataframe with the new aggregated columns added on.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    \n",
    "    for u_list in grouping_categories:\n",
    "        for grouping_function in grouping_functions:\n",
    "            new_col_name = \"_\".join(u_list) + \"_\" + grouping_function\n",
    "            if grouping_function == 'nunique':\n",
    "                grp = df2[u_list].groupby(by=u_list[0:len(u_list)-1])[u_list[len(u_list)-1]].nunique().reset_index().\\\n",
    "                    rename(index=str, columns={u_list[len(u_list)-1]:new_col_name})\n",
    "                df2 = df2.merge(grp, on=u_list[0:len(u_list)-1], how='left')\n",
    "            elif grouping_function == 'cumcount':\n",
    "                grp = df2[u_list].groupby(by=u_list[0:len(u_list)-1])[u_list[len(u_list)-1]].cumcount()\n",
    "                df2[new_col_name] = grp.values\n",
    "            else:\n",
    "                raise ValueError(f\"That grouping function {grouping_function} is not currently supported.  Use nunique and/or cumcount.\")\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bin_column(df: pd.DataFrame, collist: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Log bins the feature columns given in collist\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.  Copied - not changed.\n",
    "        collist (List[str]): List of columns to log bin, as strings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of the input dataframe with the given columns log-binned.  \n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    for col in collist:\n",
    "        df2[col] = np.log2(1 + df2[col].values).astype(int)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_click(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Adds the 'next_click' feature to a dataframe\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.  Copied - not changed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of the input dataframe with the 'next_click' feature added.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_num_categories = 2**26 # max number of categories in our hash\n",
    "    df2 = df.copy()\n",
    "    df2['user_hash'] = (df2['ip'].astype(str) + \"_\" + df2['app'].astype(str) + \"_\" + df2['device'].astype(str) \\\n",
    "            + \"_\" + df2['os'].astype(str)).apply(hash) % max_num_categories\n",
    "    click_buffer = np.full(max_num_categories, 3000000000, dtype=np.uint32)\n",
    "    df2['epoch_time'] = df2['click_time'].astype(np.int64) // 10**9 # Get epoch time of each click\n",
    "    \n",
    "    next_clicks = [] # Empty list to be filled for next click by user hash\n",
    "    # This loop goes backwards through each user by time, gets the time of their next click\n",
    "    for userhash, time in zip(reversed(df2['user_hash'].values), reversed(df2['epoch_time'].values)):\n",
    "        next_clicks.append(click_buffer[userhash] - time)\n",
    "        click_buffer[userhash] = time\n",
    "    # Since we went through backwards, reverse the next clicks and add it as a column\n",
    "    df2['next_click'] = list(reversed(next_clicks))\n",
    "    \n",
    "    # Last clicks in each user hash have high values as we'll do 3000000000 - (click_time) so we need to address this. \n",
    "    # We'll write a function to log-bin features and use it within this one.  Separate as we want it for other columns too.\n",
    "    df2 = log_bin_column(df2, ['next_click'])\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
